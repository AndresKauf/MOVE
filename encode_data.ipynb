{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode data\n",
    "\n",
    "This notebook runs part of the Multi-Omics Variational autoEncoder (MOVE) framework for using the structure the VAE has identified for extracting categorical data assositions across all continuous datasets. In the MOVE paper we used it for identifiying drug assosiations in clinical and multi-omics data. This part is a guide for encoding the data that can be used as input in MOVE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for encoding\n",
    "\n",
    "def encode_cat(raw_input, num_classes=None, uniques=None, na='NA'):\n",
    "   matrix = np.array(raw_input)\n",
    "   n_labels = matrix.shape[1]\n",
    "   n_samples = matrix.shape[0]\n",
    "   \n",
    "   # make endocding dict\n",
    "   encodings = defaultdict(dict)\n",
    "   count = 0\n",
    "   no_unique = 0\n",
    "   \n",
    "   if uniques is None:\n",
    "      no_unique = 1\n",
    "      encodings = defaultdict(dict)\n",
    "      for lab in range(0,n_labels):\n",
    "         uniques = np.unique(matrix[:,lab])\n",
    "         uniques = sorted(uniques)\n",
    "         num_classes = len(uniques[uniques != na])\n",
    "         count = 0\n",
    "         for u in uniques:\n",
    "            if u == na:\n",
    "               encodings[lab][u] = np.zeros(num_classes)\n",
    "               continue\n",
    "            encodings[lab][u] = np.zeros(num_classes)\n",
    "            encodings[lab][u][count] = 1\n",
    "            count += 1\n",
    "   else:\n",
    "      for u in uniques:\n",
    "         if u == na:\n",
    "            encodings[u] = np.zeros(num_classes)\n",
    "            continue\n",
    "         encodings[u] = np.zeros(num_classes)\n",
    "         encodings[u][count] = 1\n",
    "         count += 1\n",
    "   \n",
    "   # encode the data\n",
    "   data_input = np.zeros((n_samples,n_labels,num_classes))\n",
    "   i = 0\n",
    "   for patient in matrix:\n",
    "      \n",
    "      data_sparse = np.zeros((n_labels, num_classes))\n",
    "      \n",
    "      count = 0\n",
    "      for lab in patient:\n",
    "         if no_unique == 1:\n",
    "            data_sparse[count] = encodings[count][lab]\n",
    "         else:\n",
    "            if lab != na:\n",
    "               lab = int(float(lab))\n",
    "            data_sparse[count] = encodings[lab]\n",
    "         count += 1\n",
    "      \n",
    "      data_input[i] = data_sparse\n",
    "      i += 1\n",
    "      \n",
    "   return data_input\n",
    "\n",
    "def encode_con(raw_input):\n",
    "   \n",
    "   matrix = np.array(raw_input)\n",
    "   consum = matrix.sum(axis=1)\n",
    "   \n",
    "   data_input = np.log2(matrix + 1)\n",
    "   \n",
    "   # remove 0 variance\n",
    "   std = np.nanstd(data_input, axis=0)\n",
    "   mask_col = std != 0\n",
    "   data_input = data_input[:,mask_col]\n",
    "   \n",
    "   # z-score normalize\n",
    "   mean = np.nanmean(data_input, axis=0)\n",
    "   std = np.nanstd(data_input, axis=0)\n",
    "   \n",
    "   data_input = data_input\n",
    "   data_input -= mean\n",
    "   data_input /= std\n",
    "\n",
    "def sort_data(data, ids, labels):\n",
    "   n_labels = len(labels)\n",
    "   sorted_data = list()\n",
    "   for ids in ids:\n",
    "      if ids in data:\n",
    "         sorted_data.append(data[ids])\n",
    "      else:\n",
    "         #tmp = np.zeros((n_labels))\n",
    "         #tmp[:] = np.nan\n",
    "         tmp = [0]*n_labels\n",
    "         sorted_data.append(tmp)\n",
    "   \n",
    "   return sorted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For encoding the data you need to have each dataset/data type in a format for N x M, where N is the numer of samples/individuals and M is the number of features. For using the dataset specific weighting in the training of the VAE you need to process the datasets individually or split them when you read them in. The continuous data is z-score normalised and the categorical data is one-hot encoded. Below is an example of processing a continuous dataset and two categorical datasets with different number of categories. To ensure the correct order the ID's are used for sorting the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ids to sort the data\n",
    "ids = list()\n",
    "with open(path + \"data/baseline_ids.txt\", \"r\") as f:\n",
    "   for line in f:\n",
    "      ids.append(line.rstrip())\n",
    "\n",
    "# Encode continuous\n",
    "raw_input = dict()\n",
    "with open(path + \"data/baseline_transcriptomics.tsv\", \"r\") as f:\n",
    "   header = f.readline()\n",
    "   for line in f:\n",
    "      line = line.rstrip()\n",
    "      tmp = np.array(line.split(\"\\t\"))\n",
    "      vals = tmp[1:]\n",
    "      vals[vals == 'NA'] = np.nan\n",
    "      vals = list(map(float, vals))\n",
    "      raw_input[tmp[0]] = vals\n",
    "\n",
    "\n",
    "header = header.split(\"\\t\")\n",
    "\n",
    "sorted_data = sort_data(np.array(raw_input), ids, header)\n",
    "data_input, mask = encode_con(np.array(sorted_data))\n",
    "np.save(path + \"data/baseline_transcriptomics.npy\", sorted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical data with two categories\n",
    "raw_input = list()\n",
    "with open(path + \"data/baseline_drugs.tsv\", \"r\") as f:\n",
    "   header = f.readline()\n",
    "   for line in f:\n",
    "      line = line.rstrip()\n",
    "      tmp = line.split(\"\\t\")\n",
    "      raw_input.append(tmp[1:])\n",
    "\n",
    "header = header.split(\"\\t\")\n",
    "\n",
    "# Set the number of classes and categories\n",
    "num_classes = 2\n",
    "uniques = [0, 1, 'nan']\n",
    "\n",
    "# Sort and encode the data\n",
    "sorted_data = sort_data(np.array(raw_input), ids, header)\n",
    "data_input = encode_cat(np.array(sorted_data), num_classes, uniques, 'nan')\n",
    "np.save(path + \"data/baseline_drugs.npy\", data_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical data with three categories\n",
    "raw_input = list()\n",
    "with open(path + \"data/diabetes_genotypes_all.tsv\", \"r\") as f:\n",
    "   header = f.readline()\n",
    "   for line in f:\n",
    "      line = line.rstrip()\n",
    "      tmp = line.split(\"\\t\")\n",
    "      raw_input.append(tmp[1:])\n",
    "\n",
    "\n",
    "header = header.split(\"\\t\")\n",
    "\n",
    "# Set the number of classes and categories\n",
    "num_classes = 3\n",
    "uniques = [0, 1, 2, 'nan']\n",
    "\n",
    "# Sort and encode the data\n",
    "sorted_data = sort_data(raw_input, ids, header)\n",
    "data_input = encode_cat(np.array(sorted_data), num_classes, uniques, 'nan')\n",
    "np.save(path + \"data/diabetes_genotypes.npy\", data_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
