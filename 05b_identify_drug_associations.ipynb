{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting drug associations (Bayesian)\n",
    "\n",
    "This notebook runs part of the Multi-Omics Variational autoEncoder (MOVE) framework for using the structure the VAE has identified for extracting categorical data assositions across all continuous datasets.\n",
    "\n",
    "Similar to notebook #5, here we use MOVE to extract associations between the categorical data (drug status in this case) and the continuous omics datasets. However, here we follow an approach based on Bayesian decision theory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import move\n",
    "from move.data.dataloaders import make_dataloader\n",
    "from move.models import VAE\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_recon(model, dataloader):\n",
    "    batch = iter(dataloader).next()  # there is only 1 batch\n",
    "    batch = torch.cat(batch, 1).float().to(model.device)\n",
    "    _, recon_con, _, _ = model(batch)\n",
    "    return recon_con"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we fetch the data and make perturbations by changing the drug status of all patients to 1 (i.e., took the drug). We are analyzing 20 drugs, so we repeat this process for each drug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = None  # has to be defined\n",
    "\n",
    "cat_list, cat_names, con_list, con_names, *_ = move.utils.data_utils.get_data()\n",
    "atc_data = pd.read_csv(path, index_col=None)\n",
    "\n",
    "drug_data = cat_list[-1]  # drug dataset is last (not always)\n",
    "ndrugs = drug_data.shape[1]\n",
    "drug_mask = ~(drug_data[:, :, 1] == 1)  # select who took a drug (complement did not take drug)\n",
    "\n",
    "nsamples = drug_data.shape[0]\n",
    "\n",
    "_, dataloader = make_dataloader(cat_list=cat_list, con_list=con_list)\n",
    "\n",
    "ncontinuous = dataloader.dataset.con_all.shape[1]\n",
    "con_shapes = dataloader.dataset.con_shapes\n",
    "\n",
    "ncategorical = dataloader.dataset.cat_all.shape[1]\n",
    "cat_shapes = dataloader.dataset.cat_shapes\n",
    "\n",
    "dataloaders = []\n",
    "\n",
    "# Perturb dataset\n",
    "for i in range(ndrugs):\n",
    "    perturbed_drug = np.copy(cat_list[-1])  # drug dataset is last on cat_list\n",
    "    perturbed_drug[:,i,:] = [1, 0]  # make the perturbation\n",
    "    perturbed_cat_list = cat_list[:-1] + [perturbed_drug]  # replace with perturbed dataset\n",
    "    # generate and save new dataloader\n",
    "    _, perturbed_dataloader = make_dataloader(cat_list=perturbed_cat_list, con_list=con_list)\n",
    "    dataloaders.append(perturbed_dataloader)\n",
    "    # update drug mask, we have to consider ATC subgroups\n",
    "    atc_subgroups = atc_data[lambda df: df.idx == i][\"atc_subgroup\"].tolist()\n",
    "    linked_drug_ids = atc_data[lambda df: df.atc_subgroup.isin(atc_subgroups)][\"idx\"].tolist()\n",
    "    if len(linked_drug_ids) == 1:\n",
    "        continue\n",
    "    # select patients who took (or have no info on) a drug in the same ATC subgroup\n",
    "    #   complement of all who did not take any drug in the same ATC\n",
    "    atc_drug_mask = ~np.all(drug_data[:, linked_drug_ids, 1] == 1, axis=1)\n",
    "    # => took drug X or a drug in the same ATC subgroup\n",
    "    drug_mask[:, i] = atc_drug_mask\n",
    "\n",
    "dataloaders.append(dataloader)\n",
    "\n",
    "orig_con = dataloaders[-1].dataset.con_all\n",
    "nan_mask = (orig_con == 0).numpy()  # NaN values were transformed to 0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will calculate the reconstruction of the baseline data and the reconstruction of the perturbed data. The method can be summarized as:\n",
    "\n",
    "1. Load pretrained model (see previous notebooks for model training and hyperparameter tuning)\n",
    "2. Obtain reconstruction for the original and perturbed data\n",
    "3. Calculate difference between reconstructions and average across # refits (we ensemble different number of refits for benchmarking)\n",
    "4. Calculate Bayes factor $K = \\log{p(x_\\text{perturbed} > x_0)} - \\log{p(x_\\text{perturbed} \\leq x_0)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# define # steps at which we do calculations\n",
    "plot_steps = [1, 2, 3, 4, 5, 6, 8, 10, 12, 15, 17, 20, 22, 25, 27, 30, 35, 40, 45, 50]\n",
    "n_refits = plot_steps[-1]\n",
    "\n",
    "eps = 1e-8\n",
    "mean_diff = torch.zeros((ndrugs, nsamples, ncontinuous)).to(device)\n",
    "bayes_k = np.empty((len(plot_steps), ndrugs, ncontinuous))\n",
    "\n",
    "j = 0\n",
    "for i in range(n_refits):\n",
    "    ## Fetch trained model\n",
    "    model = VAE(categorical_shapes=cat_shapes, continuous_shapes=con_shapes, nhiddens=[2000], num_latent=nlatent,\n",
    "                beta=0.0001, cat_weights=[1,1,1], con_weights=[2,1,1,1,1,1,1], dropout=0.1, cuda=False).to(device)\n",
    "\n",
    "    state_dict_path = path # path to saved checpoint\n",
    "    model.load_state_dict(torch.load(state_dict_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    ## Calculate reconstructions\n",
    "\n",
    "    # Get reconstruction from baseline model\n",
    "    base_recon = get_recon(model, dataloaders[-1])\n",
    "    # Add differences [D x N x F]\n",
    "    for k in range(ndrugs):\n",
    "        diff = (get_recon(model, dataloaders[k]) - base_recon)\n",
    "        mean_diff[k, :, :] += diff\n",
    "\n",
    "    try: # only do calculation every plot step\n",
    "        j = plot_steps.index(i + 1)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    avg_div = plot_steps[j]\n",
    "\n",
    "    for drug_idx in range(ndrugs):\n",
    "        # select individuals who took the drug (originally), shape: N x 1\n",
    "        samples_mask = drug_mask[:, [drug_idx]]\n",
    "        # average by number of refits\n",
    "        diff = mean_diff[drug_idx, :, :] / avg_div  # shape: N x F\n",
    "        diff = np.ma.array(diff.numpy(), mask=samples_mask | nan_mask)  # mask individuals who took drug and NaN values\n",
    "        # perturbed > baseline => perturbed - baseline > 0\n",
    "        prob = np.mean(diff > 1e-8, axis=0).data  # shape: F\n",
    "        k = np.log(prob + eps) - np.log(1 - prob + eps)\n",
    "        bayes_k[j, drug_idx, :] = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the Bayes factors to rank features (the higher the factor the more significant the feature), and then we\n",
    "use the cumulative evidence to establish a false discovery rate (FDR). Finally, by setting a threshold on the FDR\n",
    "(e.g., 0.05), we select the significant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_shapes = [0] + [x.shape[1] for x in con_list]\n",
    "con_labels = [\"clinical\", \"diet_wearables\", \"proteomics\", \"target_metabolomics\", \"untarget_metabolomics\", \"transcriptomics\", \"metagenomics\"]\n",
    "\n",
    "con_df = pd.DataFrame(dict(x=reduce(list.__add__, con_names))).reset_index()\n",
    "con_df.columns = [\"feature_id\", \"feature_name\"]\n",
    "\n",
    "drug_df = pd.DataFrame(dict(x=cat_names[-1])).reset_index()\n",
    "drug_df.columns = [\"drug_id\", \"drug_name\"]\n",
    "\n",
    "fdr_threshold = 0.05\n",
    "\n",
    "ks = np.abs(bayes_k)\n",
    "probas = np.exp(ks) / (1 + np.exp(ks))\n",
    "\n",
    "for n in plot_steps:\n",
    "    j = plot_steps.index(n)\n",
    "    sort_ids = np.argsort(ks[j, :, :], axis=None)[::-1]\n",
    "    p = np.take(probas[j, :, :], sort_ids)\n",
    "\n",
    "    fdr = np.cumsum(1 - p) / np.arange(1, p.size + 1)\n",
    "    idx = np.argmin(np.abs(fdr - fdr_threshold), axis=0)\n",
    "\n",
    "    colname = str(fdr_threshold).replace(\".\", \"_\")\n",
    "    sig_ids = sort_ids[:idx]\n",
    "    sig_ids = np.vstack((sig_ids // ncontinuous, sig_ids % ncontinuous)).T\n",
    "    df = pd.DataFrame(sig_ids, columns=[\"drug_id\", \"feature_id\"]).sort_values(\"drug_id\")\n",
    "    df = df.merge(drug_df, on=\"drug_id\").merge(con_df, on=\"feature_id\")\n",
    "    df[\"dataset\"] = pd.cut(df.feature_id, bins=np.cumsum(con_shapes), right=False, labels=con_labels)\n",
    "    df.to_csv(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('02456-project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90afd891bd49a08a504b9df7771ccf48672e102a3f9596b1dc4be29d6a15d8d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
