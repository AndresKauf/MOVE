{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode data\n",
    "\n",
    "This notebook runs part of the Multi-Omics Variational autoEncoder (MOVE) framework for using the structure the VAE has identified for extracting categorical data assositions across all continuous datasets. In the MOVE paper we used it for identifiying drug assosiations in clinical and multi-omics data. This part is a guide for encoding the data that can be used as input in MOVE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for encoding\n",
    "\n",
    "def encode_cat(raw_input, num_classes=None, uniques=None, na='NA'):\n",
    "   matrix = np.array(raw_input)\n",
    "   n_labels = matrix.shape[1]\n",
    "   n_samples = matrix.shape[0]\n",
    "   \n",
    "   # make endocding dict\n",
    "   encodings = defaultdict(dict)\n",
    "   count = 0\n",
    "   no_unique = 0\n",
    "   \n",
    "\n",
    "\n",
    "#    if uniques is None:\n",
    "#       no_unique = 1\n",
    "#       encodings = defaultdict(dict)\n",
    "#       for lab in range(0,n_labels):\n",
    "#          uniques = np.unique(matrix[:,lab])\n",
    "#          uniques = sorted(uniques)\n",
    "#          num_classes = len(uniques[uniques != na])\n",
    "#          count = 0\n",
    "#          for u in uniques:\n",
    "#             if u == na:\n",
    "#                encodings[lab][u] = np.zeros(num_classes)\n",
    "#                continue\n",
    "#             encodings[lab][u] = np.zeros(num_classes)\n",
    "#             encodings[lab][u][count] = 1\n",
    "#             count += 1\n",
    "#    else:\n",
    "   for u in uniques:\n",
    "      if u == na:\n",
    "         encodings[u] = np.zeros(num_classes)\n",
    "         continue\n",
    "      encodings[u] = np.zeros(num_classes)\n",
    "      encodings[u][count] = 1\n",
    "      count += 1\n",
    "\n",
    "   # encode the data\n",
    "   data_input = np.zeros((n_samples,n_labels,num_classes))\n",
    "   i = 0\n",
    "   for patient in matrix:\n",
    "      \n",
    "      data_sparse = np.zeros((n_labels, num_classes))\n",
    "      count = 0\n",
    "      for lab in patient:\n",
    "         if no_unique == 1:\n",
    "            data_sparse[count] = encodings[count][lab]\n",
    "         else:\n",
    "            if lab != na:\n",
    "               lab = int(float(lab))\n",
    "            data_sparse[count] = encodings[lab]\n",
    "         count += 1\n",
    "      \n",
    "      data_input[i] = data_sparse\n",
    "      i += 1\n",
    "      \n",
    "   return data_input\n",
    "\n",
    "def encode_con(raw_input):\n",
    "   \n",
    "   matrix = np.array(raw_input)\n",
    "   consum = matrix.sum(axis=1)\n",
    "   \n",
    "   data_input = np.log2(matrix + 1)\n",
    "   \n",
    "   # remove 0 variance\n",
    "   std = np.nanstd(data_input, axis=0)\n",
    "   mask_col = std != 0\n",
    "   data_input = data_input[:,mask_col]\n",
    "   # z-score normalize\n",
    "   mean = np.nanmean(data_input, axis=0)\n",
    "   \n",
    "   std = np.nanstd(data_input, axis=0)\n",
    "   \n",
    "#    data_input = data_input  #check: data_input=data_input\n",
    "   data_input -= mean\n",
    "   data_input /= std\n",
    "   return data_input, mask_col  # Added return function\n",
    "\n",
    "def sort_data(data, ids, labels):\n",
    "   n_labels = len(labels)\n",
    "   sorted_data = list()\n",
    "\n",
    "   for _ids in ids: #check: ids/ids\n",
    "\n",
    "      if _ids in data:\n",
    "         sorted_data.append(data[_ids])\n",
    "      else:\n",
    "         #tmp = np.zeros((n_labels))\n",
    "         #tmp[:] = np.nan\n",
    "         tmp = [0]*n_labels\n",
    "         sorted_data.append(tmp)\n",
    "   return sorted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(data_type):\n",
    "    ids = list()\n",
    "    with open(path + \"data/baseline_ids.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            ids.append(line.rstrip()) \n",
    "             \n",
    "    raw_input = dict()\n",
    "    with open(path + f\"data/{data_type}.tsv\", \"r\") as f:\n",
    "        header = f.readline()\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            tmp = np.array(line.split(\"\\t\"))\n",
    "            vals = tmp[1:]\n",
    "            vals[vals == 'NA'] = np.nan\n",
    "            vals = list(map(float, vals))\n",
    "            raw_input[tmp[0]] = vals\n",
    "    header = header.split(\"\\t\")\n",
    "    \n",
    "    return ids, raw_input, header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For encoding the data you need to have each dataset/data type in a format for N x M, where N is the numer of samples/individuals and M is the number of features. For using the dataset specific weighting in the training of the VAE you need to process the datasets individually or split them when you read them in. The continuous data is z-score normalised and the categorical data is one-hot encoded. Below is an example of processing a continuous dataset and two categorical datasets with different number of categories. To ensure the correct order the ID's are used for sorting the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_cat_file(data_type): #Todo make separate get IDs\n",
    "\n",
    "    ids, raw_input, header = read_files(data_type)\n",
    "    \n",
    "    # Set the number of classes and categories\n",
    "    sorted_data = sort_data(raw_input, ids, header)\n",
    "    unique_sorted_data = np.unique(sorted_data)\n",
    "    num_classes = len(unique_sorted_data[~np.isnan(unique_sorted_data)])\n",
    "    \n",
    "    uniques = [*range(num_classes), 'nan']\n",
    "    \n",
    "    data_input = encode_cat(sorted_data, num_classes, uniques, 'nan')\n",
    "    np.save(path + f\"data/{data_type}.npy\", data_input)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_con_file(data_type):\n",
    "\n",
    "    ids, raw_input, header = read_files(data_type)\n",
    "    \n",
    "    sorted_data = sort_data(raw_input, ids, header)\n",
    "    data_input, mask = encode_con(sorted_data)\n",
    "    np.save(path + f\"data/{data_type}.npy\", sorted_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_cat_file('diabetes_genotypes')\n",
    "# generate_cat_file('baseline_drugs')\n",
    "# generate_cat_file('baseline_categorical')\n",
    "\n",
    "# generate_con_file('baseline_continuous')\n",
    "# generate_con_file('baseline_transcriptomics')\n",
    "# generate_con_file('baseline_diet_wearables')\n",
    "# generate_con_file('baseline_proteomic_antibodies')\n",
    "# generate_con_file('baseline_target_metabolomics')\n",
    "# generate_con_file('baseline_untarget_metabolomics')\n",
    "# generate_con_file('baseline_metagenomics')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
