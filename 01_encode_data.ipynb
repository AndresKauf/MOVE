{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode data\n",
    "\n",
    "This notebook runs part of the Multi-Omics Variational autoEncoder (MOVE) framework for using the structure the VAE has identified for extracting categorical data assositions across all continuous datasets. In the MOVE paper we used it for identifiying drug assosiations in clinical and multi-omics data. This part is a guide for encoding the data that can be used as input in MOVE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_bad = pd.read_csv('./data/baseline_metagenomics.tsv', sep='\\t', header=0)\n",
    "file_good = pd.read_csv('./data/baseline_untarget_metabolomics.tsv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(file_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for encoding\n",
    "\n",
    "def encode_cat(raw_input, num_classes=None, uniques=None, na='NA'):\n",
    "   matrix = np.array(raw_input)\n",
    "   print(f'unique: {np.unique(matrix)}')\n",
    "   n_labels = matrix.shape[1]\n",
    "   n_samples = matrix.shape[0]\n",
    "   \n",
    "   # make endocding dict\n",
    "   encodings = defaultdict(dict)\n",
    "   count = 0\n",
    "   no_unique = 0\n",
    "   \n",
    "   if uniques is None:\n",
    "      no_unique = 1\n",
    "      encodings = defaultdict(dict)\n",
    "      for lab in range(0,n_labels):\n",
    "         uniques = np.unique(matrix[:,lab])\n",
    "         uniques = sorted(uniques)\n",
    "         num_classes = len(uniques[uniques != na])\n",
    "         count = 0\n",
    "         for u in uniques:\n",
    "            if u == na:\n",
    "               encodings[lab][u] = np.zeros(num_classes)\n",
    "               continue\n",
    "            encodings[lab][u] = np.zeros(num_classes)\n",
    "            encodings[lab][u][count] = 1\n",
    "            count += 1\n",
    "   else:\n",
    "      for u in uniques:\n",
    "         if u == na:\n",
    "            encodings[u] = np.zeros(num_classes)\n",
    "            continue\n",
    "         encodings[u] = np.zeros(num_classes)\n",
    "         encodings[u][count] = 1\n",
    "         count += 1\n",
    "   \n",
    "   # encode the data\n",
    "   data_input = np.zeros((n_samples,n_labels,num_classes))\n",
    "   i = 0\n",
    "   for patient in matrix:\n",
    "      \n",
    "      data_sparse = np.zeros((n_labels, num_classes))\n",
    "      count = 0\n",
    "      for lab in patient:\n",
    "         if no_unique == 1:\n",
    "            data_sparse[count] = encodings[count][lab]\n",
    "         else:\n",
    "            if lab != na:\n",
    "               lab = int(float(lab))\n",
    "            data_sparse[count] = encodings[lab]\n",
    "         count += 1\n",
    "      \n",
    "      data_input[i] = data_sparse\n",
    "      i += 1\n",
    "      \n",
    "   return data_input\n",
    "\n",
    "def encode_con(raw_input):\n",
    "   \n",
    "   matrix = np.array(raw_input)\n",
    "   print(f'unique: {np.unique(matrix)}')\n",
    "   consum = matrix.sum(axis=1)\n",
    "   \n",
    "   data_input = np.log2(matrix + 1)\n",
    "   \n",
    "   # remove 0 variance\n",
    "   std = np.nanstd(data_input, axis=0)\n",
    "   mask_col = std != 0\n",
    "   data_input = data_input[:,mask_col]\n",
    "   # z-score normalize\n",
    "   mean = np.nanmean(data_input, axis=0)\n",
    "   \n",
    "   std = np.nanstd(data_input, axis=0)\n",
    "   \n",
    "   data_input = data_input  #check: data_input=data_input\n",
    "   data_input -= mean\n",
    "   data_input /= std\n",
    "   return data_input, mask_col  # Added return function\n",
    "\n",
    "def sort_data(data, ids, labels):\n",
    "   n_labels = len(labels)\n",
    "   sorted_data = list()\n",
    "\n",
    "   for ids in ids: #check: ids/ids\n",
    "\n",
    "      if ids in data:\n",
    "         sorted_data.append(data[ids])\n",
    "      else:\n",
    "         #tmp = np.zeros((n_labels))\n",
    "         #tmp[:] = np.nan\n",
    "         tmp = [0]*n_labels\n",
    "         sorted_data.append(tmp)  #With random data just puts everything to zeros\n",
    "   return sorted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For encoding the data you need to have each dataset/data type in a format for N x M, where N is the numer of samples/individuals and M is the number of features. For using the dataset specific weighting in the training of the VAE you need to process the datasets individually or split them when you read them in. The continuous data is z-score normalised and the categorical data is one-hot encoded. Below is an example of processing a continuous dataset and two categorical datasets with different number of categories. To ensure the correct order the ID's are used for sorting the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "In generate_cat_file() provided num_classes does not correspond to given dataset for diabetes_genotypes datatype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-86552d234eae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mgenerate_cat_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'diabetes_genotypes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;31m# generate_cat_file('baseline_drugs', 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# generate_cat_file('baseline_categorical', 5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-86552d234eae>\u001b[0m in \u001b[0;36mgenerate_cat_file\u001b[0;34m(data_type, num_classes)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_uniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m        \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m        \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'In generate_cat_file() provided num_classes does not correspond to given dataset for {data_type} datatype'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'unique: {np.unique(sorted_data)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: In generate_cat_file() provided num_classes does not correspond to given dataset for diabetes_genotypes datatype"
     ]
    }
   ],
   "source": [
    "def generate_cat_file(data_type, num_classes): #Todo make separate get IDs\n",
    "    ids = list()\n",
    "    with open(path + \"data/baseline_ids.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            ids.append(line.rstrip()) \n",
    "             \n",
    "    raw_input = dict()\n",
    "    with open(path + f\"data/{data_type}.tsv\", \"r\") as f:\n",
    "        header = f.readline()\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            tmp = np.array(line.split(\"\\t\"))\n",
    "            vals = tmp[1:]\n",
    "            vals[vals == 'NA'] = np.nan\n",
    "            vals = list(map(float, vals))\n",
    "            raw_input[tmp[0]] = vals\n",
    "    header = header.split(\"\\t\")\n",
    "    \n",
    "    # Set the number of classes and categories\n",
    "    \n",
    "    if num_classes==2:\n",
    "        uniques = [0, 1, 'nan']\n",
    "    elif num_classes==3:\n",
    "        uniques = [0, 1, 2, 'nan']\n",
    "    elif num_classes==4:\n",
    "        uniques = [0, 1, 2, 3, 'nan']\n",
    "    elif num_classes==5:\n",
    "        uniques = [0, 1, 2, 3, 4, 'nan'] #add as input, or raise value error \n",
    "    \n",
    "\n",
    "    # Sort and encode the data\n",
    "    sorted_data = sort_data(raw_input, ids, header)\n",
    "    \n",
    "     # Set the number of classes and raise error if doesn't match\n",
    "        \n",
    "    _uniques = [*range(num_classes)]\n",
    "    uniques = [*range(num_classes), 'nan']\n",
    "#     uniques_real = \n",
    "    if np.array_equal(np.array(_uniques), np.unique(np.array(sorted_data))):\n",
    "        print('equal1')\n",
    "    if np.array_equal(np.array(uniques), np.unique(np.array(sorted_data))):\n",
    "        print('equal2')\n",
    "    \n",
    "    if not np.array_equal(np.array(_uniques), np.unique(np.array(sorted_data))) and \\\n",
    "       not np.array_equal(np.array(uniques), np.unique(np.array(sorted_data))):\n",
    "       raise ValueError(f'In generate_cat_file() provided num_classes does not correspond to given dataset for {data_type} datatype')\n",
    "    \n",
    "    print(f'unique: {np.unique(sorted_data)}')\n",
    "    data_input = encode_cat(sorted_data, num_classes, uniques, 'nan')\n",
    "    np.save(path + f\"data/{data_type}.npy\", data_input)    \n",
    "    \n",
    "    \n",
    "generate_cat_file('diabetes_genotypes', 3)\n",
    "generate_cat_file('baseline_drugs', 2)\n",
    "generate_cat_file('baseline_categorical', 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_con_file(data_type):\n",
    "    ids = list()\n",
    "    with open(path + \"data/baseline_ids.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            ids.append(line.rstrip())\n",
    "\n",
    "    # Encode continuous\n",
    "    raw_input = dict()\n",
    "    with open(path + f\"data/{data_type}.tsv\", \"r\") as f:\n",
    "        header = f.readline()\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            tmp = np.array(line.split(\"\\t\"))\n",
    "            vals = tmp[1:]\n",
    "            vals[vals == 'NA'] = np.nan\n",
    "            \n",
    "            vals = list(map(float, vals))\n",
    "            raw_input[tmp[0]] = vals\n",
    "\n",
    "    header = header.split(\"\\t\")\n",
    "    sorted_data = sort_data(raw_input, ids, header)\n",
    "\n",
    "    data_input, mask = encode_con(sorted_data)\n",
    "    np.save(path + f\"data/{data_type}.npy\", sorted_data)\n",
    "\n",
    "generate_con_file('baseline_continuous')\n",
    "generate_con_file('baseline_transcriptomics')\n",
    "generate_con_file('baseline_diet_wearables')\n",
    "generate_con_file('baseline_proteomic_antibodies')\n",
    "generate_con_file('baseline_target_metabolomics')\n",
    "generate_con_file('baseline_untarget_metabolomics')\n",
    "generate_con_file('baseline_metagenomics')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
