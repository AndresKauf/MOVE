{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode data\n",
    "\n",
    "This notebook runs part of the Multi-Omics Variational autoEncoder (MOVE) framework for using the structure the VAE has identified for extracting categorical data assositions across all continuous datasets. In the MOVE paper we used it for identifiying drug assosiations in clinical and multi-omics data. This part is a guide for encoding the data that can be used as input in MOVE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for encoding\n",
    "\n",
    "def encode_cat(raw_input, na='NA'):\n",
    "   \"\"\"\n",
    "   Encodes categorical data into one-hot encoding\n",
    "   \n",
    "   inputs:\n",
    "       raw_input: a list of source data sorted by IDs from baseline_ids.txt file\n",
    "   returns:\n",
    "       data_input: one hot encoded data\n",
    "   \"\"\" \n",
    "    \n",
    "   matrix = np.array(raw_input)\n",
    "   n_labels = matrix.shape[1]\n",
    "   n_samples = matrix.shape[0]\n",
    "   \n",
    "   unique_sorted_data = np.unique(raw_input)\n",
    "   num_classes = len(unique_sorted_data[~np.isnan(unique_sorted_data)])\n",
    "   uniques = [*range(num_classes), 'nan']\n",
    " \n",
    "   # make endocding dict\n",
    "   encodings = defaultdict(dict)\n",
    "   count = 0\n",
    "   no_unique = 0\n",
    "   \n",
    "   for u in uniques:\n",
    "      if u == na:\n",
    "         encodings[u] = np.zeros(num_classes)\n",
    "         continue\n",
    "      encodings[u] = np.zeros(num_classes)\n",
    "      encodings[u][count] = 1\n",
    "      count += 1\n",
    "\n",
    "   # encode the data\n",
    "   data_input = np.zeros((n_samples,n_labels,num_classes))\n",
    "   i = 0\n",
    "   for patient in matrix:\n",
    "      \n",
    "      data_sparse = np.zeros((n_labels, num_classes))\n",
    "      count = 0\n",
    "      for lab in patient:\n",
    "         if no_unique == 1:\n",
    "            data_sparse[count] = encodings[count][lab]\n",
    "         else:\n",
    "            if lab != na:\n",
    "               lab = int(float(lab))\n",
    "            data_sparse[count] = encodings[lab]\n",
    "         count += 1\n",
    "      \n",
    "      data_input[i] = data_sparse\n",
    "      i += 1\n",
    "      \n",
    "   return data_input\n",
    "\n",
    "def encode_con(raw_input):\n",
    "   \"\"\"\n",
    "   Log transforms and z-normalizes the data\n",
    "   \n",
    "   Input: \n",
    "       raw_input: a list of source data sorted by IDs from baseline_ids.txt file\n",
    "   Returns:\n",
    "       data_input: numpy array with log transformed and z-score normalized data\n",
    "       mask_col: a np.array vector of Bolean values that correspond to nonzero sd values \n",
    "   \"\"\"\n",
    "\n",
    "   matrix = np.array(raw_input)\n",
    "   consum = matrix.sum(axis=1)\n",
    "   \n",
    "   data_input = np.log2(matrix + 1) \n",
    "   \n",
    "   # remove 0 variance\n",
    "   std = np.nanstd(data_input, axis=0)\n",
    "   mask_col = std != 0\n",
    "   data_input = data_input[:,mask_col]\n",
    "    \n",
    "   # z-score normalize\n",
    "   mean = np.nanmean(data_input, axis=0)\n",
    "   std = np.nanstd(data_input, axis=0)\n",
    "   data_input -= mean\n",
    "   data_input /= std\n",
    "   return data_input, mask_col \n",
    "\n",
    "\n",
    "def sort_data(data, ids, labels):\n",
    "    \"\"\"\n",
    "    Sorts data based on the ids file\n",
    "    \n",
    "    Inputs:\n",
    "        data: a dictionary with the data to encode\n",
    "        ids: a list of personal identfiers (ID) from baseline_ids.txt file\n",
    "        labels: a list of column names from the source data file\n",
    "    Returns:\n",
    "        sorted_data: a list of source data sorted by IDs from baseline_ids.txt file\n",
    "    \"\"\"\n",
    "\n",
    "    n_labels = len(labels)\n",
    "    sorted_data = list()\n",
    "\n",
    "    for _ids in ids: #check: ids/ids\n",
    "      if _ids in data:\n",
    "         sorted_data.append(data[_ids])\n",
    "      else:\n",
    "         tmp = [0]*n_labels\n",
    "         sorted_data.append(tmp)\n",
    "    return sorted_data\n",
    "\n",
    "def read_files(path, data_type, ids_file_name, na):\n",
    "    \"\"\"\n",
    "    Function reads the input file into the dictionary\n",
    "    \n",
    "    Inputs:\n",
    "        data_type: a string that defines a name of .tsv file to encode\n",
    "        na: a string that defines how NA values are defined in the source data file\n",
    "    Returns:\n",
    "        ids: a list of personal identfiers (ID) from baseline_ids.txt file\n",
    "        raw_input: a dictionary with the data to encode\n",
    "        header: a list of column names from the source data file\n",
    "    \"\"\"\n",
    "    \n",
    "    ids = list()\n",
    "    with open(path + f\"{ids_file_name}.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            ids.append(line.rstrip()) \n",
    "             \n",
    "    raw_input = dict()\n",
    "    with open(path + f\"{data_type}.tsv\", \"r\") as f:\n",
    "        header = f.readline()\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            tmp = np.array(line.split(\"\\t\"))\n",
    "            vals = tmp[1:]\n",
    "            vals[vals == na] = np.nan\n",
    "            vals = list(map(float, vals))\n",
    "            raw_input[tmp[0]] = vals\n",
    "    header = header.split(\"\\t\")\n",
    "    \n",
    "    return ids, raw_input, header\n",
    "\n",
    "def generate_file(var_type, path, data_type, ids_file_name, na='NA'):\n",
    "    \"\"\"\n",
    "    Function encodes source data type and saves the file\n",
    "    \n",
    "    inputs:\n",
    "        var_type: a string out of ['categorical', 'continuous'], defines input data type to encode\n",
    "        path: a string that defines a path to the directory the input data is stored\n",
    "        data_type: a string that defines a name of .tsv file to encode\n",
    "        na: a string that defines how NA values are defined in the source data file\n",
    "    \"\"\"\n",
    "    \n",
    "    ids, raw_input, header = read_files(path, data_type, ids_file_name, na)\n",
    "    sorted_data = sort_data(raw_input, ids, header)\n",
    "    \n",
    "    if var_type == 'categorical':\n",
    "        data_input = encode_cat(sorted_data, 'nan')\n",
    "    elif var_type == 'continuous':\n",
    "        data_input, _ = encode_con(sorted_data)\n",
    "    \n",
    "    np.save(path + f\"{data_type}.npy\", data_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For encoding the data you need to have each dataset/data type in a format for N x M, where N is the numer of samples/individuals and M is the number of features. For using the dataset specific weighting in the training of the VAE you need to process the datasets individually or split them when you read them in. The continuous data is z-score normalised and the categorical data is one-hot encoded. Below is an example of processing a continuous dataset and two categorical datasets with different number of categories. To ensure the correct order the ID's are used for sorting the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded diabetes_genotypes\n",
      "Encoded baseline_drugs\n",
      "Encoded baseline_categorical\n",
      "Encoded baseline_continuous\n",
      "Encoded baseline_transcriptomics\n",
      "Encoded baseline_diet_wearables\n",
      "Encoded baseline_proteomic_antibodies\n",
      "Encoded baseline_target_metabolomics\n",
      "Encoded baseline_untarget_metabolomics\n",
      "Encoded baseline_metagenomics\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Reads the data as dictionary\n",
    "    with open(r'data.yaml') as file:\n",
    "        data_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    \n",
    "    # Takes variables from the read file\n",
    "    path = data_dict['path']\n",
    "    ids_file_name = data_dict['ids_file_name']\n",
    "    na_encoding = data_dict['na_encoding']\n",
    "\n",
    "    # Encodes categorical data\n",
    "    for cat_data in data_dict['categorical_data_files']:\n",
    "        generate_file('categorical', path, \n",
    "                      cat_data, ids_file_name, na_encoding)\n",
    "        print(f'Encoded {cat_data}')\n",
    "    \n",
    "    # Encodes continuous data \n",
    "    for con_data in data_dict['continuous_data_files']:\n",
    "        generate_file('continuous', path, con_data, ids_file_name, na_encoding)    \n",
    "        print(f'Encoded {con_data}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
