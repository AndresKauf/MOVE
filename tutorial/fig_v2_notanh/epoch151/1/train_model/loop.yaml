optimizer_config:
  _target_: torch.optim.adam.Adam
  lr: 0.0003
  weight_decay: 0.0
lr_scheduler_config: null
max_epochs: 151
max_grad_norm: null
annealing_epochs: 25
annealing_function: linear
annealing_schedule: monotonic
prog_every_n_epoch: 10
log_grad: false
